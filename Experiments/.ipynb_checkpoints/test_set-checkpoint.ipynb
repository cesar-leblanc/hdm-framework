{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdd4f51",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2d053",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb34d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f9e20",
   "metadata": {},
   "source": [
    "## Paths creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "framework_directory = os.path.abspath(os.path.join(notebook_directory, '..'))\n",
    "\n",
    "sys.path.append(framework_directory)\n",
    "\n",
    "print(framework_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc31ceb",
   "metadata": {},
   "source": [
    "## Species data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5415acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_occurrences = ['sample_id', 'preferred_taxon', 'domin']\n",
    "columns_abundance = ['domin', 'midPoint']\n",
    "\n",
    "npms_occurrences = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/occurrences.csv'), usecols=columns_occurrences)\n",
    "npms_abundance = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/domin_scores.csv'), usecols=columns_abundance)\n",
    "\n",
    "npms_occurrences = npms_occurrences.rename(columns={\"sample_id\": \"PlotObservationID\", \"preferred_taxon\": \"Matched concept\", \"domin\": \"Domin scale\"})\n",
    "npms_abundance = npms_abundance.rename(columns={\"domin\": \"Domin scale\", \"midPoint\": \"Cover\"})\n",
    "\n",
    "npms_occurrences = npms_occurrences.merge(npms_abundance, how='left')  # Add cover\n",
    "npms_occurrences['Cover %'] = npms_occurrences['Cover'] * 100  # Add cover percentage\n",
    "npms_occurrences.drop(['Domin scale', 'Cover'], axis=1, inplace=True)  # Remove unwanted columns\n",
    "npms_occurrences = npms_occurrences.dropna()  # Remove rows with missing information\n",
    "\n",
    "npms_occurrences = npms_occurrences.reset_index(drop=True)\n",
    "npms_occurrences_esy = npms_occurrences.copy()\n",
    "\n",
    "npms_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80572e",
   "metadata": {},
   "source": [
    "## GBIF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess_data import add_gbif_normalization\n",
    "\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(framework_directory, 'Data/eva_to_gbif_species.pkl'), 'rb') as f:\n",
    "    eva_to_gbif_species = pickle.load(f)\n",
    "\n",
    "original_value_counts = npms_occurrences['PlotObservationID'].value_counts()\n",
    "npms_occurrences, _ = add_gbif_normalization(npms_occurrences, eva_to_gbif_species)\n",
    "new_value_counts = npms_occurrences['PlotObservationID'].value_counts()\n",
    "rows_to_remove = [value for value, count in original_value_counts.items() if value in new_value_counts and new_value_counts[value] < count * 0.75]\n",
    "npms_occurrences = npms_occurrences[~npms_occurrences['PlotObservationID'].isin(rows_to_remove)]\n",
    "npms_occurrences = npms_occurrences[npms_occurrences['PlotObservationID'].map(new_value_counts) >= 1]\n",
    "\n",
    "npms_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3483f",
   "metadata": {},
   "source": [
    "## Header data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_attributes = ['sample_id', 'caption', 'text_value']\n",
    "columns_localisation = ['id', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "npms_attributes = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/sample_attributes.csv'), usecols=columns_attributes)\n",
    "npms_localisation = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/sample_info.csv'), usecols=columns_localisation)\n",
    "npms_habitat_lookup = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/npms_habitat_lookup.csv'))\n",
    "\n",
    "npms_attributes = npms_attributes.rename(columns={\"sample_id\": \"PlotObservationID\", \"text_value\": \"Habitat\"})\n",
    "npms_localisation = npms_localisation.rename(columns={\"id\": \"PlotObservationID\", \"LONGITUDE\": \"Longitude\", \"LATITUDE\": \"Latitude\"})\n",
    "npms_habitat_lookup = npms_habitat_lookup.rename(columns={\"NPMS_broad_habitat\": \"Broad habitat\", \"NPMS_fine-scale_habitat\": \"Fine-scale habitat\"})\n",
    "\n",
    "npms_attributes = npms_attributes[npms_attributes['caption'] == 'NPMS Habitat']  # Keep rows with habitat\n",
    "npms_attributes = npms_attributes.merge(npms_localisation, how='left')  # Add location information\n",
    "npms_attributes = npms_attributes[['PlotObservationID', 'Longitude', 'Latitude', 'Habitat']]  # Change columns order\n",
    "grouped_habitats = npms_habitat_lookup.groupby(\"Broad habitat\")[\"Fine-scale habitat\"].nunique()  # Group the habitats by broad habitats\n",
    "filtered_habitats = grouped_habitats[grouped_habitats == 1].index  # Retrieve names of broad habitats that contain exactly one fine-scale habitat\n",
    "npms_habitat_lookup = npms_habitat_lookup[npms_habitat_lookup[\"Broad habitat\"].isin(filtered_habitats)]  # Keep only those broad habitats in the habitat lookup info\n",
    "npms_attributes = npms_attributes.merge(npms_habitat_lookup, left_on=\"Habitat\", right_on=\"Broad habitat\", how=\"left\")  # Merge the attributes DataFrame with the habitat lookup info DataFrame\n",
    "npms_attributes[\"Habitat\"] = npms_attributes[\"Fine-scale habitat\"].fillna(npms_attributes[\"Habitat\"])  # Replace broad habitats that contain only one fine-scale habitats by their fine-scale habitat\n",
    "npms_attributes = npms_attributes.drop([\"Broad habitat\", \"Fine-scale habitat\"], axis=1)  # Drop unwanted columns\n",
    "npms_attributes = npms_attributes[npms_attributes['Habitat'] != 'Not in scheme']  # Remove rows when the habitat is not in the scheme\n",
    "npms_attributes = npms_attributes.dropna()  # Remove rows with missing information\n",
    "npms_attributes = npms_attributes.reset_index(drop=True)  # Reset the DataFrame index\n",
    "\n",
    "npms_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d44a2",
   "metadata": {},
   "source": [
    "## EUNIS code conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10933b41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_codes = ['EUNIS 2020 code', 'EUNIS 2007 code']\n",
    "\n",
    "codes = pd.read_excel(os.path.join(framework_directory, 'Datasets/eunis_habitats.xlsx'), usecols=columns_codes)\n",
    "code_dict = {}\n",
    "\n",
    "for index, row in codes.iterrows():\n",
    "    codes_2007 = str(row['EUNIS 2007 code']).split(';')\n",
    "    for code_2007 in codes_2007:\n",
    "        code_2007 = code_2007.strip()\n",
    "        if code_2007 == 'nan':\n",
    "            continue\n",
    "        code_2020 = str(row['EUNIS 2020 code'])\n",
    "        if code_2007 in code_dict:\n",
    "            code_dict[code_2007].append(code_2020)\n",
    "        else:\n",
    "            code_dict[code_2007] = [code_2020]\n",
    "\n",
    "for code_2007, code_2020 in code_dict.items():\n",
    "    print(code_2007, \"->\", code_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e68d4",
   "metadata": {},
   "source": [
    "## Header data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef2c7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_habitats = ['NPMS', 'EUNIS']\n",
    "npms_habitats = pd.read_csv(os.path.join(framework_directory, 'Datasets/NPMS/NPMS_EUNIS.csv'), usecols=columns_habitats)\n",
    "\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "\n",
    "npms_attributes['NPMS'] = npms_attributes['Habitat'].copy()\n",
    "npms_attributes = npms_attributes.rename(columns={\"Habitat\": \"EUNIS\"})\n",
    "npms_attributes = npms_attributes[npms_attributes['EUNIS'].isin(npms_habitats['NPMS'].values)]\n",
    "npms_attributes.loc[:, 'EUNIS'] = npms_attributes['EUNIS'].map(npms_habitats.set_index('NPMS')['EUNIS'])\n",
    "\n",
    "for index, row in npms_attributes.iterrows():\n",
    "    old_codes = str(row['EUNIS']).split(', ')\n",
    "    new_codes = []\n",
    "    for old_code in old_codes:\n",
    "        old_code = old_code.strip()\n",
    "        if old_code in code_dict:\n",
    "            new_codes.extend(code_dict[old_code])\n",
    "        else:\n",
    "            new_codes.append('~')\n",
    "    npms_attributes.at[index, 'EUNIS'] = ', '.join(new_codes)\n",
    "\n",
    "for index, row in npms_attributes.iterrows():\n",
    "    codes_list = row['EUNIS'].split(', ')  # Split codes by comma\n",
    "    new_codes = []\n",
    "    for code in codes_list:\n",
    "        if code in codes['EUNIS 2020 code'].values:\n",
    "            if code.startswith('MA2') and len(code) != 5:  # Replace code starting with 'MA2' and length not equal to 5\n",
    "                if code.startswith('MA2') and len(code) == 3:\n",
    "                    new_codes += list(codes[codes['EUNIS 2020 code'].str.startswith(code[:3]) & (codes['EUNIS 2020 code'].str.len() == 5)]['EUNIS 2020 code'])\n",
    "                elif code.startswith('MA2') and len(code) == 4:\n",
    "                    new_codes += list(codes[codes['EUNIS 2020 code'].str.startswith(code[:4]) & (codes['EUNIS 2020 code'].str.len() == 5)]['EUNIS 2020 code'])\n",
    "            elif not code.startswith('MA2') and len(code) != 3:  # Replace code with length not equal to 3\n",
    "                if not code.startswith('MA2') and len(code) == 1:\n",
    "                    new_codes += list(codes[codes['EUNIS 2020 code'].str.startswith(code[:1]) & (codes['EUNIS 2020 code'].str.len() == 3)]['EUNIS 2020 code'])\n",
    "                elif not code.startswith('MA2') and len(code) == 2:\n",
    "                    new_codes += list(codes[codes['EUNIS 2020 code'].str.startswith(code[:2]) & (codes['EUNIS 2020 code'].str.len() == 3)]['EUNIS 2020 code'])\n",
    "            else:\n",
    "                new_codes.append(code)  # Keep original code if length is correct\n",
    "        else:\n",
    "            new_codes.append(code)  # Keep original code if not present in codes DataFrame\n",
    "\n",
    "    npms_attributes.at[index, 'EUNIS'] = ', '.join(new_codes)\n",
    "\n",
    "npms_attributes['noise percentage'] = npms_attributes['EUNIS'].apply(lambda x: sum(label == '~' or label not in le_header.classes_ for label in x.split(', ')) / len(x.split(', ')) * 100)\n",
    "npms_attributes = npms_attributes[npms_attributes['noise percentage'] <= 50]\n",
    "npms_attributes = npms_attributes.drop('noise percentage', axis=1)\n",
    "npms_attributes.loc[:, 'EUNIS'] = npms_attributes['EUNIS'].str.split(', ') # Split the codes into a list in each row\n",
    "npms_attributes.loc[:, 'EUNIS'] = npms_attributes['EUNIS'].apply(lambda x: list(set(x))) # Remove duplicate codes in each row\n",
    "npms_attributes.loc[:, 'EUNIS'] = npms_attributes['EUNIS'].apply(lambda x: [code for code in x if code != '~' and code in le_header.classes_])\n",
    "npms_attributes.loc[:, 'EUNIS'] = npms_attributes['EUNIS'].apply(lambda x: ', '.join(sorted(x)))  # Join the codes back into a single string in each row\n",
    "npms_attributes = npms_attributes[npms_attributes['PlotObservationID'].isin(npms_occurrences['PlotObservationID'])]\n",
    "npms_attributes = npms_attributes[npms_attributes['EUNIS'].str.count(',') < 5]  # Only keep vegetation plots corresponding to 5 or less EUNIS habitats\n",
    "npms_attributes = npms_attributes.dropna()\n",
    "npms_attributes = npms_attributes.reset_index(drop=True)\n",
    "npms_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7aad54",
   "metadata": {},
   "source": [
    "## Species data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670ca49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npms_occurrences = npms_occurrences[npms_occurrences['PlotObservationID'].isin(npms_attributes['PlotObservationID'])]\n",
    "npms_occurrences = npms_occurrences.reset_index(drop=True)\n",
    "npms_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fff29e",
   "metadata": {},
   "source": [
    "## EUNIS-ESy test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "npms_occurrences_esy = npms_occurrences_esy[npms_occurrences_esy['PlotObservationID'].isin(npms_occurrences['PlotObservationID'])]\n",
    "npms_occurrences_esy, _ = add_gbif_normalization(npms_occurrences_esy, None)\n",
    "\n",
    "npms_occurrences_esy = npms_occurrences_esy.rename(columns={'PlotObservationID': 'RELEVE_NR', 'Matched concept': 'TaxonName', 'Cover %': 'Cover_Perc'})\n",
    "npms_occurrences_esy['Cover_Perc'] = npms_occurrences_esy['Cover_Perc'].astype(int)\n",
    "\n",
    "npms_occurrences_esy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f78e2",
   "metadata": {},
   "source": [
    "## Target values retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = npms_attributes['EUNIS'].values\n",
    "npms_attributes.drop(['EUNIS', 'NPMS'], axis=1, inplace=True)  # Remove unwanted columns\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3501a5b",
   "metadata": {},
   "source": [
    "## Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "npms_attributes.to_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'), index=False, sep='\\t')\n",
    "npms_occurrences.to_csv(os.path.join(framework_directory, 'Datasets/test_species.csv'), index=False, sep='\\t')\n",
    "npms_occurrences_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "with open(os.path.join(framework_directory, 'Data/test_values.txt'), 'w') as file:\n",
    "    for value in y:\n",
    "        file.write(str(value) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
