{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491d2623",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a51f9f",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from sklearn.cluster import KMeans\n",
    "import contextily as ctx\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66458ad7",
   "metadata": {},
   "source": [
    "## Paths creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "framework_directory = os.path.abspath(os.path.join(notebook_directory, '..'))\n",
    "\n",
    "sys.path.append(framework_directory)\n",
    "\n",
    "print(framework_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1106cd",
   "metadata": {},
   "source": [
    "## Time–performance characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\"MLP\", \"RFC\", \"XGB\", \"TNC\", \"FTT\"] * 2\n",
    "\n",
    "mlp_training_accuracies = [97.82, 97.88, 97.87, 97.73, 97.82, 97.72, 97.80, 97.64, 97.71, 97.72]\n",
    "rfc_training_accuracies = [99.26, 99.26, 99.26, 99.26, 99.26, 99.26, 99.26, 99.26, 99.26, 99.26]\n",
    "xgb_training_accuracies = [99.26, 99.26, 99.23, 99.25, 99.24, 99.26, 99.24, 99.25, 99.26, 99.26]\n",
    "tnc_training_accuracies = [86.95, 86.23, 86.40, 86.71, 85.94, 86.36, 86.01, 85.90, 86.78, 86.93]\n",
    "ftt_training_accuracies = [96.14, 96.26, 96.16, 96.14, 96.54, 96.52, 96.00, 96.18, 96.16, 96.00]\n",
    "accuracy = [np.mean(mlp_training_accuracies), np.mean(rfc_training_accuracies), np.mean(xgb_training_accuracies), np.mean(tnc_training_accuracies), np.mean(ftt_training_accuracies)] + [37.42, 38.24, 36.75, 36.59, 35.71]\n",
    "\n",
    "mlp_training_times = [9193.57, 9111.05, 9277.34, 9226.49, 9377.13, 9282.78, 9323.88, 8691.53, 9244.66, 9173.71]\n",
    "rfc_training_times = [2283.03, 2280.12, 2278.89, 2337.44, 2275.51, 2332.83, 2312.09, 2290.51, 2275.04, 2293.37]\n",
    "xgb_training_times = [53811.31, 54621.94, 53881.44, 54926.00, 54850.32, 54543.94, 54685.12, 54283.54, 54034.83, 54879.43]\n",
    "tnc_training_times = [7787.45, 7364.54, 6167.79, 10989.60, 7021.01, 6635.65, 5719.83, 7344.03, 8014.12, 10486.35]\n",
    "ftt_training_times = [11008.77, 11461.27, 11393.96, 11269.92, 11260.76, 10791.19, 11442.52, 11144.11, 11126.66, 10478.45]\n",
    "time = [np.mean(mlp_training_times), np.mean(rfc_training_times), np.mean(xgb_training_times), np.mean(tnc_training_times), np.mean(ftt_training_times)] + [0.42, 0.62, 28.24, 4.43, 2.30]\n",
    "\n",
    "standard_deviation = [np.std(mlp_training_accuracies), np.std(rfc_training_accuracies), np.std(xgb_training_accuracies), np.std(tnc_training_accuracies), np.std(ftt_training_accuracies)]\n",
    "standard_deviation = [((x - min(standard_deviation)) / (max(standard_deviation) - min(standard_deviation))) + 1 for x in standard_deviation]\n",
    "number_parameters = [2172628, 200, 200, 1315210, 1239372]\n",
    "number_parameters = [((x - min(number_parameters)) / (max(number_parameters) - min(number_parameters))) + 1 for x in number_parameters]\n",
    "size = standard_deviation + number_parameters\n",
    "\n",
    "pipeline = [\"Training\"] * 5 + [\"Prediction\"] * 5\n",
    "\n",
    "df = pd.DataFrame(list(zip(model, time, accuracy, size, pipeline)),\n",
    "               columns =['Model', 'Time', 'Accuracy', 'Size', 'Pipeline'])\n",
    "\n",
    "fig = px.scatter(df, x=\"Time\", y=\"Accuracy\", color=\"Model\",\n",
    "                 size='Size', facet_col='Pipeline',\n",
    "                 text=\"Model\", labels={\"Time\": \"Time (s)\", \"Accuracy\": \"Accuracy (%)\"})\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "fig.update_traces(textposition='bottom center',\n",
    "                  marker=dict(line=dict(width=2,\n",
    "                                        color='Black')))\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=2000,\n",
    "    showlegend=False, \n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "fig.update_xaxes(matches=None, )\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/time–performance_characteristic.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9db24b",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "epochs = list(range(1, 101))\n",
    "values = [88.8699, 89.9944, 90.9012, 91.2493, 91.753, 91.9377, 92.0493, 92.2255, 92.2432, 92.4019, 92.3454, 92.5178, 92.5326, 92.8125, 92.7535, 92.8742, 92.8202, 92.8393, 92.8905, 92.7829, 92.9184, 92.9879, 92.9368, 92.9946, 93.0307, 93.0823, 93.106, 93.1555, 93.1434, 93.0361, 93.1622, 93.0456, 93.1882, 93.1678, 93.2963, 93.1519, 93.1102, 93.1803, 93.2212, 93.1467, 93.2695, 95.4257, 95.9578, 96.2402, 96.3902, 96.5103, 96.6018, 96.6814, 96.7414, 96.8154, 96.8526, 96.9051, 96.9651, 96.9938, 97.0473, 97.0858, 97.1216, 97.1505, 97.1843, 97.2018, 97.2446, 97.2592, 97.2745, 97.3146, 97.3346, 97.3554, 97.3842, 97.4067, 97.4106, 97.433, 97.4537, 97.4652, 97.4634, 97.5021, 97.5126, 97.5373, 97.5499, 97.5478, 97.5692, 97.589, 97.5984, 97.6089, 97.6309, 97.6377, 97.6367, 97.6603, 97.6662, 97.6578, 97.6898, 97.6819, 97.702, 97.6992, 97.7158, 97.7294, 97.7484, 97.7416, 97.7403, 97.7713, 97.7668, 97.7726]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs,\n",
    "    y=values\n",
    "))\n",
    "\n",
    "fig.add_vrect(x0=epochs[0], x1=epochs[41],\n",
    "    label=dict(text=\"LR: 0.001\", textposition=\"top center\", font=dict(size=15, family=\"Times New Roman\"),),\n",
    "    fillcolor=\"#1f77b4\", opacity=0.25, line_width=0)\n",
    "\n",
    "fig.add_vrect(x0=epochs[41], x1=epochs[-1],\n",
    "    label=dict(text=\"LR: 0.0001\", textposition=\"top center\", font=dict(size=15, family=\"Times New Roman\"),),\n",
    "    fillcolor=\"#ff7f0e\", opacity=0.25, line_width=0)\n",
    "\n",
    "fig.add_annotation(x=epochs[-1], y=values[-1],\n",
    "            text=\"Early stopping did not occur\",\n",
    "            showarrow=True,\n",
    "            arrowsize=3, \n",
    "            arrowwidth=1,\n",
    "            arrowhead=1,\n",
    "            ax=-50,\n",
    "            ay=50)\n",
    "\n",
    "fig.add_vline(x=epochs[41], line_width=3, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    showlegend=False,\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Accuracy (%)'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[epochs[0], epochs[-1]])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/accuracy.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a85bf",
   "metadata": {},
   "source": [
    "## Species per plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_species = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_species.csv'))\n",
    "eva_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'))\n",
    "\n",
    "counter_less_10 = 0\n",
    "counter_11_30 = 0\n",
    "counter_more_31 = 0\n",
    "\n",
    "counter_31_40 = 0\n",
    "counter_41_50 = 0\n",
    "counter_51_60 = 0\n",
    "counter_more_61 = 0\n",
    "\n",
    "count_species = eva_species.groupby('PlotObservationID').count()\n",
    "for plot_id in eva_header['PlotObservationID'].to_list():\n",
    "    if count_species.loc[plot_id]['Matched concept'] <= 10:\n",
    "        counter_less_10 += 1\n",
    "    elif count_species.loc[plot_id]['Matched concept'] >= 11 and count_species.loc[plot_id]['Matched concept'] <= 30:\n",
    "        counter_11_30 +=1\n",
    "    else:\n",
    "        counter_more_31 += 1\n",
    "        if count_species.loc[plot_id]['Matched concept'] >= 31 and count_species.loc[plot_id]['Matched concept'] <= 40:\n",
    "            counter_31_40 +=1\n",
    "        elif count_species.loc[plot_id]['Matched concept'] >= 41 and count_species.loc[plot_id]['Matched concept'] <= 50:\n",
    "            counter_41_50 += 1\n",
    "        elif count_species.loc[plot_id]['Matched concept'] >= 51 and count_species.loc[plot_id]['Matched concept'] <= 60:\n",
    "            counter_51_60 += 1  \n",
    "        else:\n",
    "            counter_more_61 += 1\n",
    "\n",
    "counter_31_40 = round(counter_31_40 / counter_more_31, 2)  \n",
    "counter_41_50 = round(counter_41_50 / counter_more_31, 2)\n",
    "counter_51_60 = round(counter_51_60 / counter_more_31, 2)\n",
    "counter_more_61 = round(counter_more_61 / counter_more_31, 2)\n",
    "\n",
    "counter_less_10 = round(counter_less_10 / len(eva_header), 2)\n",
    "counter_11_30 = round(counter_11_30 / len(eva_header), 2)\n",
    "counter_more_31 = round(counter_more_31 / len(eva_header), 2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "ax1.set_title('All plots')\n",
    "overall_ratios = [counter_more_31, counter_11_30, counter_less_10]\n",
    "labels = ['More than 31', 'Between 11 and 30', 'Less than 10']\n",
    "explode = [0.1, 0, 0]\n",
    "angle = -180 * overall_ratios[0]\n",
    "wedges, *_ = ax1.pie(overall_ratios, autopct='%1.1f%%', startangle=angle,\n",
    "                     labels=labels, explode=explode)\n",
    "\n",
    "age_ratios = [counter_31_40, counter_41_50, counter_51_60, counter_more_61]\n",
    "age_labels = ['Under 40', '41-50', '51-60', 'Over 61']\n",
    "bottom = 1\n",
    "width = .2\n",
    "\n",
    "for j, (height, label) in enumerate(reversed([*zip(age_ratios, age_labels)])):\n",
    "    bottom -= height\n",
    "    bc = ax2.bar(0, height, width, bottom=bottom, color='C0', label=label,\n",
    "                 alpha=0.1 + 0.25 * j)\n",
    "    ax2.bar_label(bc, labels=[f\"{height:.0%}\"], label_type='center')\n",
    "\n",
    "ax2.set_title('Manifold plots')\n",
    "ax2.legend()\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(- 2.5 * width, 2.5 * width)\n",
    "\n",
    "theta1, theta2 = wedges[0].theta1, wedges[0].theta2\n",
    "center, r = wedges[0].center, wedges[0].r\n",
    "bar_height = sum(age_ratios)\n",
    "\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, bar_height), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "con.set_linewidth(4)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, 0), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(4)\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/species_per_plot.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0b3ca",
   "metadata": {},
   "source": [
    "## Plots by habitat group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a636d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'), usecols=['Expert System'])\n",
    "\n",
    "eva_header['Habitat type'] = eva_header['Expert System'].apply(lambda x: x[:3] if x.startswith('MA2') else x[0])\n",
    "eva_header = eva_header.groupby(['Habitat type', 'Expert System']).size().reset_index(name='Count')\n",
    "eva_header['Habitat type count'] = eva_header.groupby('Habitat type')['Count'].transform('sum')\n",
    "eva_header = eva_header.sort_values(by=['Habitat type count', 'Count'], ascending=[False, False])\n",
    "\n",
    "colors = []\n",
    "color_scales = [px.colors.sequential.Reds, px.colors.sequential.Blues, px.colors.sequential.Greens, px.colors.sequential.Oranges,\n",
    "                px.colors.sequential.Purples, px.colors.sequential.Greys, px.colors.sequential.YlOrBr, px.colors.sequential.RdPu]\n",
    "\n",
    "number_of_habitats = eva_header['Habitat type'].value_counts(sort=False).values\n",
    "\n",
    "for i in range(len(number_of_habitats)):\n",
    "    color_scale = color_scales[i][2:] * 10\n",
    "    colors.extend(color_scale[:number_of_habitats[i]])\n",
    "\n",
    "fig = px.bar(eva_header, x='Habitat type', y='Count', color='Expert System',\n",
    "             color_discrete_sequence=colors)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/habitat_type.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b603c1",
   "metadata": {},
   "source": [
    "## Observation of species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d587c51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eva_species = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_species.csv'), usecols=['Matched concept'])\n",
    "\n",
    "eva_species = eva_species.groupby('Matched concept').size().reset_index(name='Count')\n",
    "eva_species = eva_species.sort_values(by='Count', ascending=False)\n",
    "\n",
    "fig = px.line(eva_species, x=\"Matched concept\", y=\"Count\", log_y=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    xaxis_title='Species',\n",
    "    xaxis = dict(\n",
    "        tickmode = 'linear',\n",
    "        tick0 = 0,\n",
    "        dtick = 1000\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/species.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10036a42",
   "metadata": {},
   "source": [
    "## Ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7]\n",
    "x_labels = [\"All\", \"Species + Location\", \"Species + Environmental\", \"Location + Environmental\", \"Species\", \"Location\", \"Environmental\"]\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# Line 1\n",
    "y1 = [88.7437, 88.0000, 88.7580, 21.4009, 87.8343, 12.7983, 19.8927]\n",
    "std_1 = [0.3514, 0.3693, 0.3668, 0.8639, 0.3358, 1.0368, 0.7909]\n",
    "y1_upper = [x + y for x, y in zip(y1, std_1)]\n",
    "y1_lower = [x - y for x, y in zip(y1, std_1)]\n",
    "y1_lower = y1_lower[::-1]\n",
    "\n",
    "# Line 2\n",
    "y2 = [79.3921, 79.6920, 79.4754, 24.3493, 79.0983, 22.9658, 15.1615]\n",
    "std_2 = [0.4478, 0.4765, 0.4571, 0.7567, 0.4757, 0.6759, 0.6180]\n",
    "y2_upper = [x + y for x, y in zip(y2, std_2)]\n",
    "y2_lower = [x - y for x, y in zip(y2, std_2)]\n",
    "y2_lower = y2_lower[::-1]\n",
    "\n",
    "# Line 3\n",
    "y3 = [86.8039, 86.6270, 86.8484, 26.5687, 86.0186, 24.0786, 19.4963]\n",
    "std_3 = [0.4524, 0.4254, 0.4598, 0.9531, 0.4686, 0.6943, 0.9863]\n",
    "y3_upper = [x + y for x, y in zip(y3, std_3)]\n",
    "y3_lower = [x - y for x, y in zip(y3, std_3)]\n",
    "y3_lower = y3_lower[::-1]\n",
    "\n",
    "# Line 4\n",
    "y4 = [80.2178, 79.3296, 80.2858, 21.4163, 79.2267, 16.7509, 19.5843]\n",
    "std_4 = [0.5378, 1.0547, 0.5652, 1.2249, 0.5257, 0.8521, 1.0226]\n",
    "y4_upper = [x + y for x, y in zip(y4, std_4)]\n",
    "y4_lower = [x - y for x, y in zip(y4, std_4)]\n",
    "y4_lower = y4_lower[::-1]\n",
    "\n",
    "# Line 5\n",
    "y5 = [86.9774, 86.1416, 87.0251, 18.2139, 86.0481, 13.3969, 17.8147]\n",
    "std_5 = [0.3791, 0.3617, 0.3547, 0.7813, 0.3298, 1.4059, 0.8566]\n",
    "y5_upper = [x + y for x, y in zip(y5, std_5)]\n",
    "y5_lower = [x - y for x, y in zip(y5, std_5)]\n",
    "y5_lower = y5_lower[::-1]\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y1_upper+y1_lower,\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(31, 119, 180, 0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y2_upper+y2_lower,\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(255, 127, 14, 0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y3_upper+y3_lower,\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(44, 160, 44, 0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y4_upper+y4_lower,\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(214, 39, 40, 0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y5_upper+y5_lower,\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(148, 103, 189, 0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y1,\n",
    "    line_color='rgb(31, 119, 180)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y2,\n",
    "    line_color='rgb(255, 127, 14)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y3,\n",
    "    line_color='rgb(44, 160, 44)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y4,\n",
    "    line_color='rgb(214, 39, 40)',\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y5,\n",
    "    line_color='rgb(148, 103, 189)',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=x,\n",
    "        ticktext=x_labels,\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0.5, 7.5], title_text='Used features')\n",
    "fig.update_yaxes(title_text='Accuracy (%)')\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.75, y=y1[0] + 1, text='MLP',\n",
    "    showarrow=False, font=dict(color='rgb(31, 119, 180)', size=20)\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=0.75, y=y2[0] - 1, text='RFC',\n",
    "    showarrow=False, font=dict(color='rgb(255, 127, 14)', size=20)\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=0.75, y=y3[0] - 1, text='XGB',\n",
    "    showarrow=False, font=dict(color='rgb(44, 160, 44)', size=20)\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=0.75, y=y4[0] + 1, text='TNC',\n",
    "    showarrow=False, font=dict(color='rgb(214, 39, 40)', size=20)\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=0.75, y=y5[0] + 1, text='FTT',\n",
    "    showarrow=False, font=dict(color='rgb(148, 103, 189)', size=20)\n",
    ")\n",
    "\n",
    "fig.update_traces(mode='lines')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/ablation_study.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b00f5",
   "metadata": {},
   "source": [
    "## Distribution EVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'))\n",
    "\n",
    "data = eva_header[['Latitude', 'Longitude']]\n",
    "kmeans = KMeans(n_clusters=50, n_init=10)\n",
    "kmeans.fit(data)\n",
    "eva_header['Cluster'] = kmeans.labels_\n",
    "\n",
    "grouped_eva_header = eva_header.groupby('Cluster').agg({'Latitude': 'mean', 'Longitude': 'mean', 'Cluster': 'size'})\n",
    "grouped_eva_header.columns = ['Latitude', 'Longitude', 'Size']\n",
    "grouped_eva_header.reset_index(inplace=True)\n",
    "grouped_eva_header.sort_values('Cluster', inplace=True)\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "\n",
    "grouped_eva_header['Longitude'], grouped_eva_header['Latitude'] = transformer.transform(\n",
    "    grouped_eva_header['Longitude'].values,\n",
    "    grouped_eva_header['Latitude'].values\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    grouped_eva_header['Longitude'],\n",
    "    grouped_eva_header['Latitude'],\n",
    "    s=grouped_eva_header['Size'] / 10,\n",
    "    alpha=0.7,\n",
    "    c='lime',\n",
    ")\n",
    "\n",
    "margin = 0.05\n",
    "x_min, x_max = grouped_eva_header['Longitude'].min(), grouped_eva_header['Longitude'].max()\n",
    "x_range = x_max - x_min\n",
    "ax.set_xlim(x_min - margin * x_range, x_max + margin * x_range)\n",
    "\n",
    "y_min, y_max = grouped_eva_header['Latitude'].min(), grouped_eva_header['Latitude'].max()\n",
    "y_range = x_range + 2 * margin * x_range\n",
    "ax.set_ylim(((y_min + y_max) / 2) - (y_range / 4) - 1000000, ((y_min + y_max) / 2) + (y_range / 4) - 1000000)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "scatter.set_clip_on(True)\n",
    "\n",
    "for x, y, size in zip(grouped_eva_header['Longitude'], grouped_eva_header['Latitude'], grouped_eva_header['Size']):\n",
    "    text = plt.text(x, y, str(size), ha='center', va='center', fontsize=6)\n",
    "    text.set_clip_on(True)\n",
    "\n",
    "ctx.add_basemap(plt.gca(), crs='EPSG:3857', source=ctx.providers.Esri.WorldImagery, attribution='')\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/distribution_eva.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e332e",
   "metadata": {},
   "source": [
    "## Distribution NPMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'))\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "\n",
    "test_header['Longitude'], test_header['Latitude'] = transformer.transform(\n",
    "    test_header['Longitude'].values,\n",
    "    test_header['Latitude'].values\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    test_header['Longitude'], \n",
    "    test_header['Latitude'], \n",
    "    c='red',               \n",
    "    s=3\n",
    ")\n",
    "\n",
    "margin = 0.01  # Adjust the margin as needed\n",
    "y_min, y_max = test_header['Latitude'].min(), test_header['Latitude'].max()\n",
    "y_range = y_max - y_min\n",
    "ax.set_ylim(y_min - margin * y_range, y_max + margin * y_range)\n",
    "\n",
    "x_min, x_max = test_header['Longitude'].min(), test_header['Longitude'].max()\n",
    "x_range = y_range + 2 * margin * y_range  # Adjust the calculation to include the margin\n",
    "ax.set_xlim(((x_min + x_max) / 2) - (x_range / 2), ((x_min + x_max) / 2) + (x_range / 2))\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "scatter.set_clip_on(True)\n",
    "\n",
    "ctx.add_basemap(ax, crs='EPSG:3857', source=ctx.providers.OpenStreetMap.Mapnik, attribution='')\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/distribution_npms.pdf'), dpi=300, bbox_inches='tight')  # Adjust the filename and dpi as needed\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6dd86",
   "metadata": {},
   "source": [
    "## Split assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a6784",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eva_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "legend_labels = []\n",
    "\n",
    "for fold, group in eva_header.groupby('Fold'):\n",
    "    scatter = ax.scatter(\n",
    "        group['Longitude'],\n",
    "        group['Latitude'],\n",
    "        c=colors[fold],\n",
    "        s=10,\n",
    "        linewidths=0.1,\n",
    "        edgecolors='black',\n",
    "        label=f'Fold {fold}'\n",
    "    )\n",
    "    legend_labels.append(f'Fold {fold}')\n",
    "\n",
    "coordinates_montpellier = (3.876716, 43.610769)\n",
    "x_min = coordinates_montpellier[0] - 0.5\n",
    "x_max = coordinates_montpellier[0] + 0.5\n",
    "y_min = coordinates_montpellier[1] - 0.25\n",
    "y_max = coordinates_montpellier[1] + 0.25\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "for x in np.arange(0, 80, 0.0897222222224867):\n",
    "    ax.axvline(x, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "for y in np.arange(0, 80, 0.0897222222224867):\n",
    "    ax.axhline(y, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "ctx.add_basemap(ax, crs='EPSG:4326', source=ctx.providers.OpenStreetMap.Mapnik, attribution='')\n",
    "\n",
    "\n",
    "legend = ax.legend(\n",
    "    title='Folds',\n",
    "    labels=legend_labels,\n",
    "    loc='upper right',\n",
    "    fontsize='medium',\n",
    "    frameon=True\n",
    ")\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/split_assignment.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103974a9",
   "metadata": {},
   "source": [
    "## Threatened plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32188413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "habitats = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'), usecols=['Expert System'])\n",
    "red_list = pd.read_excel(os.path.join(framework_directory, 'Datasets/red_list_habitats.xlsx'), sheet_name='Terrestrial cross to EUNIS 2021', usecols=['Overall category EU28+', 'EUNIS 2019/2021 code'])\n",
    "\n",
    "replacement_dict = {\n",
    "    'Least concern': 'Least Concern',\n",
    "    'Near threatened': 'Near Threatened',\n",
    "    'Critically EndangeredR': 'Critically Endangered'\n",
    "}\n",
    "category_priority = ['Data Deficient', 'Least Concern', 'Near Threatened', 'Vulnerable', 'Endangered', 'Critically Endangered']\n",
    "category_priority_map = {category: priority for priority, category in enumerate(category_priority)}\n",
    "\n",
    "red_list = red_list.dropna()\n",
    "red_list = red_list.rename(columns={'Overall category EU28+':'Category', 'EUNIS 2019/2021 code': 'EUNIS'})\n",
    "red_list['Category'] = red_list['Category'].replace(replacement_dict)\n",
    "red_list = red_list.drop_duplicates()\n",
    "red_list['Category'] = red_list['Category'].astype(pd.CategoricalDtype(categories=category_priority, ordered=True))\n",
    "red_list = red_list.sort_values(by=['EUNIS', 'Category'], ascending=[True, False])\n",
    "red_list = red_list.drop_duplicates(subset='EUNIS', keep='first')\n",
    "red_list.reset_index(drop=True, inplace=True)\n",
    "\n",
    "eunis_to_category = red_list.set_index('EUNIS')['Category'].to_dict()\n",
    "habitats['Category'] = habitats['Expert System'].apply(lambda x: eunis_to_category.get(x, 'Data Deficient'))\n",
    "\n",
    "habitats['Group'] = habitats['Category'].apply(lambda x: 'Incomplete Category' if x == 'Data Deficient' else \n",
    "                                               'Stable Category' if x in ['Least Concern', 'Near Threatened'] else\n",
    "                                               'Threatened Category' if x in ['Vulnerable', 'Endangered', 'Critically Endangered'] else\n",
    "                                               'Unknown Category')\n",
    "\n",
    "habitats_counts = habitats.groupby(['Group', 'Category']).size().reset_index(name='Count')\n",
    "habitats_counts['Parent'] = ' '\n",
    "\n",
    "total_counts = habitats_counts['Count'].sum()\n",
    "habitats_counts['Percentage'] = (habitats_counts['Count'] / total_counts) * 100\n",
    "\n",
    "fig = px.sunburst(habitats_counts, path=['Parent', 'Group', 'Category'], values='Count', color='Group',\n",
    "                 color_discrete_map={'(?)':'white', 'Stable Category':'#00CC96', 'Incomplete Category':'#636EFA', 'Threatened Category':'#EF553B'}\n",
    "                 )\n",
    "\n",
    "fig.update_traces(textinfo=\"label+percent parent\")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/threatened_plots.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7033f0",
   "metadata": {},
   "source": [
    "## Date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dfde9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eva_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'))\n",
    "original_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/EVA/eva_header.csv'), delimiter='\\t', usecols=['PlotObservationID', 'Date of recording'])\n",
    "\n",
    "original_header['Year'] = original_header['Date of recording'].fillna('0')\n",
    "original_header = original_header.drop(['Date of recording'], axis=1)\n",
    "original_header['Year'] = original_header['Year'].str.replace(':', '', regex=True).apply(lambda x: int(x[-4:]))\n",
    "\n",
    "eva_header = eva_header.merge(original_header[['PlotObservationID', 'Year']], on='PlotObservationID', how='left')\n",
    "\n",
    "count_eva_years = eva_header['Year'].value_counts().to_frame(name=\"Occurrences\").rename(index={0: '?'})\n",
    "\n",
    "count_eva_years = count_eva_years.drop(['?'])\n",
    "\n",
    "list_of_years = count_eva_years.index.to_list()\n",
    "\n",
    "fig = px.bar(\n",
    "    count_eva_years,\n",
    "    y='Occurrences',\n",
    "    x=count_eva_years.index,\n",
    "    text='Occurrences',\n",
    "    color='Occurrences',\n",
    "    labels={\n",
    "        \"Occurrences\": \"Number of plots\",\n",
    "        \"index\": \"Year\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "fig.update_layout(height=500, width=1000)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/date_range.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05980b7",
   "metadata": {},
   "source": [
    "## Performance by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_kingdom_regions_columns = ['rgn_name', 'geometry']\n",
    "\n",
    "united_kingdom_regions = gpd.read_file(os.path.join(framework_directory, 'Datasets/united_kingdom_regions.shp'))\n",
    "united_kingdom_regions = united_kingdom_regions[united_kingdom_regions_columns]\n",
    "united_kingdom_regions = united_kingdom_regions.rename(columns={'rgn_name': 'region'})\n",
    "united_kingdom_regions['region'] = united_kingdom_regions['region'].apply(lambda x: ''.join(char for char in x if char not in [\"'\", \"[\", \"]\"]))\n",
    "united_kingdom_regions = united_kingdom_regions.sort_values(by='region').reset_index(drop=True)\n",
    "united_kingdom_regions['accuracy (%)'] = [35.98, 41.40, 87.69, 32.88, 31.79, 20.92, 33.33, 43.44, 38.77, 19.25, 47.91, 40.45]\n",
    "united_kingdom_regions.index = united_kingdom_regions['region']\n",
    "united_kingdom_regions = united_kingdom_regions.drop(['region'], axis=1)\n",
    "united_kingdom_regions = united_kingdom_regions.to_crs(crs=3857)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "united_kingdom_regions.plot(column=\"accuracy (%)\", cmap=\"viridis\", linewidth=0.8, ax=ax, legend=True)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "ctx.add_basemap(ax, crs='EPSG:3857', source=ctx.providers.OpenStreetMap.Mapnik, attribution='')\n",
    "\n",
    "cax = plt.gcf().get_axes()[1]\n",
    "cax.set_title('Accuracy (%)', fontsize=12)\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/performance_by_region.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6fe49",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Experiments/predictions.pkl'), 'rb') as file:\n",
    "    predictions = pickle.load(file)\n",
    "target_values = np.load(os.path.join(framework_directory, 'Data/target_values.npy'))\n",
    "split_assignments = np.load(os.path.join(framework_directory, 'Data/split_assignments.npy'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "\n",
    "target_values = np.concatenate([target_values[split_assignments == i] for i in range(10)])\n",
    "target_values = le_header.inverse_transform(target_values)\n",
    "target_values = np.asarray([target_value[:-1] for target_value in target_values])\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "predictions = le_header.inverse_transform(predictions)\n",
    "predictions = np.asarray([prediction[:-1] for prediction in predictions])\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(target_values, predictions, normalize=\"true\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[habitat for habitat in np.unique(np.asarray([habitat[:-1] for habitat in le_header.classes_]))])\n",
    "\n",
    "disp.plot(include_values=False, colorbar=False, ax=ax)\n",
    "\n",
    "ax.set_xticklabels(disp.display_labels, rotation=45, fontsize=8)\n",
    "ax.set_yticklabels(disp.display_labels, rotation=45, fontsize=8)\n",
    "\n",
    "plt.savefig(os.path.join(framework_directory, 'Images/confusion_matrix.pdf'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba39ff",
   "metadata": {},
   "source": [
    "## Top-20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c19b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = torch.load(os.path.join(framework_directory, 'Experiments/attributions.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_country.pkl'), 'rb') as f:\n",
    "    ohe_country = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_ecoregion.pkl'), 'rb') as f:\n",
    "    ohe_ecoregion = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_dune.pkl'), 'rb') as f:\n",
    "    ohe_dune = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_coast.pkl'), 'rb') as f:\n",
    "    ohe_coast = pickle.load(f)\n",
    "\n",
    "feature_names = le_species.classes_.tolist() + [\"Longitude\"] + [\"Latitude\"] + [\"Altitude\"] + ohe_country.categories_[0].tolist() + [f\"Ecoregion {ecoregion}\" for ecoregion in ohe_ecoregion.categories_[0].tolist()] + ohe_dune.categories_[0].tolist() + ohe_coast.categories_[0].tolist()\n",
    "\n",
    "features_attributions = torch.mean(attributions, dim=0)\n",
    "\n",
    "k = 20\n",
    "\n",
    "all_species = torch.sum(features_attributions[:len(le_species.classes_)])\n",
    "all_location = torch.sum(features_attributions[len(le_species.classes_): len(le_species.classes_) + 2])\n",
    "all_environmental = torch.sum(features_attributions[len(le_species.classes_) + 2:])\n",
    "\n",
    "sorted_indices = torch.argsort(features_attributions, descending=True)\n",
    "top_k_indices = sorted_indices[:k]\n",
    "top_k_features = [feature_names[i] for i in top_k_indices]\n",
    "top_k_scores = features_attributions[top_k_indices]\n",
    "\n",
    "trace = go.Bar(x=[\"Species\", \"Environment\", \"Location\"] + top_k_features,\n",
    "               y=torch.cat((all_species.unsqueeze(0), all_environmental.unsqueeze(0), all_location.unsqueeze(0), top_k_scores), dim=0),\n",
    "               marker=dict(color=['red'] * 3 + [\"blue\"] * 20)\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Feature'),\n",
    "    yaxis=dict(title='Mean attribution score'),\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/top_20_features.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4cf18",
   "metadata": {},
   "source": [
    "## Importance by criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d00bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = torch.load(os.path.join(framework_directory, 'Experiments/attributions.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "    \n",
    "features_attributions = torch.mean(attributions, dim=0)\n",
    "\n",
    "features_attributions_species = features_attributions[:len(le_species.classes_)]\n",
    "\n",
    "features_attributions_location = features_attributions[len(le_species.classes_): len(le_species.classes_) + 2]\n",
    "\n",
    "features_attributions_environment = features_attributions[len(le_species.classes_) + 2:]\n",
    "\n",
    "labels = ['Species','Location', 'Environment']\n",
    "values = [torch.sum(features_attributions_species), torch.sum(features_attributions_location), torch.sum(features_attributions_environment)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n",
    "                             insidetextorientation='horizontal', hole=.3, #pull=[0.2, 0, 0]\n",
    "                            )])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/criteria_importance.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be72ce",
   "metadata": {},
   "source": [
    "## Criteria importance per habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = torch.load(os.path.join(framework_directory, 'Experiments/attributions.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(columns=['Habitat type', 'Attribution score', 'Feature type'])\n",
    "\n",
    "nbr_habitats = np.zeros(8, dtype=int)\n",
    "habitat_types = ['MA2', 'N', 'Q', 'R', 'S', 'T', 'U', 'V']\n",
    "\n",
    "for code in le_header.classes_:\n",
    "    for i, habitat in enumerate(habitat_types):\n",
    "        if code.startswith(habitat):\n",
    "            nbr_habitats[i] += 1\n",
    "            break\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "for i in range(len(nbr_habitats)):\n",
    "    end_idx = start_idx + nbr_habitats[i]\n",
    "    summed_features = attributions[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "    summed_internal = torch.sum(summed_features[:, :len(le_species.classes_)], dim=1)\n",
    "    internal_df = pd.DataFrame({'Habitat type': habitat_types[i],\n",
    "                           'Attribution score': summed_internal.numpy(),\n",
    "                           'Feature type': 'Internal'})\n",
    "    \n",
    "    summed_external = torch.sum(summed_features[:, len(le_species.classes_):], dim=1)\n",
    "    external_df = pd.DataFrame({'Habitat type': habitat_types[i],\n",
    "                            'Attribution score': summed_external.numpy(),\n",
    "                            'Feature type': 'External'})\n",
    "\n",
    "    df = pd.concat([df, internal_df, external_df], ignore_index=True)\n",
    "\n",
    "pointpos_internal = [-0.4, -0.5, -0.5, -0.7, -0.9, -0.6, -0.5, -0.4]\n",
    "pointpos_external = [0.4, 0.4, 0.5, 1, 0.8, 0.9, 0.6, 0.4]\n",
    "\n",
    "show_legend = [True] + [False] * (len(pd.unique(df['Habitat type'])) - 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(pd.unique(df['Habitat type']))):\n",
    "    fig.add_trace(go.Violin(x=df['Habitat type'][(df['Feature type'] == 'Internal') &\n",
    "                                        (df['Habitat type'] == pd.unique(df['Habitat type'])[i])],\n",
    "                            y=df['Attribution score'][(df['Feature type'] == 'Internal')&\n",
    "                                               (df['Habitat type'] == pd.unique(df['Habitat type'])[i])],\n",
    "                            legendgroup='I', scalegroup='I', name='I',\n",
    "                            side='negative',\n",
    "                            pointpos=pointpos_internal[i],\n",
    "                            line_color='lightseagreen',\n",
    "                            showlegend=show_legend[i])\n",
    "             )\n",
    "    fig.add_trace(go.Violin(x=df['Habitat type'][(df['Feature type'] == 'External') &\n",
    "                                        (df['Habitat type'] == pd.unique(df['Habitat type'])[i])],\n",
    "                            y=df['Attribution score'][(df['Feature type'] == 'External')&\n",
    "                                               (df['Habitat type'] == pd.unique(df['Habitat type'])[i])],\n",
    "                            legendgroup='E', scalegroup='E', name='E',\n",
    "                            side='positive',\n",
    "                            pointpos=pointpos_external[i],\n",
    "                            line_color='mediumpurple',\n",
    "                            showlegend=show_legend[i])\n",
    "             )\n",
    "\n",
    "fig.update_traces(meanline_visible=True,\n",
    "                  points='all',\n",
    "                  jitter=0.05,\n",
    "                  scalemode='count')\n",
    "\n",
    "fig.update_layout(\n",
    "    #title_text=\"Attribution score distribution<br><i>scaled by number of habitats per habitat type\",\n",
    "    violingap=0, violingroupgap=0, violinmode='overlay',\n",
    "    width=2000,\n",
    "    height=500, \n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/criteria_importance_per_habitat.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58ade8",
   "metadata": {},
   "source": [
    "## Importance by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = torch.load(os.path.join(framework_directory, 'Experiments/attributions.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_country.pkl'), 'rb') as f:\n",
    "    ohe_country = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_ecoregion.pkl'), 'rb') as f:\n",
    "    ohe_ecoregion = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_dune.pkl'), 'rb') as f:\n",
    "    ohe_dune = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_coast.pkl'), 'rb') as f:\n",
    "    ohe_coast = pickle.load(f)\n",
    "arborescent_species = np.load(os.path.join(framework_directory, 'Datasets/arborescent_species.npy'))\n",
    "\n",
    "arborescent_species = np.where(np.isin(le_species.classes_, arborescent_species), 1, 0)\n",
    "arborescent_indices = np.where(arborescent_species == 1)[0]\n",
    "herbaceous_indices = np.where(arborescent_species == 0)[0]\n",
    "\n",
    "index = ['MA2', 'N', 'Q', 'R', 'S', 'T', 'U', 'V']\n",
    "\n",
    "scores = [[], [], [], [], [], [], [], []]\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "for i in range(len(nbr_habitats)):\n",
    "    end_idx = start_idx + nbr_habitats[i]\n",
    "    summed_features = attributions[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "    \n",
    "    summed_herbaceous = torch.sum(summed_features[:, herbaceous_indices], dim=1)\n",
    "    mean_herbaceous = torch.mean(summed_herbaceous)\n",
    "    summed_arborescent = torch.sum(summed_features[:, arborescent_indices], dim=1)\n",
    "    mean_arborescent = torch.mean(summed_arborescent)\n",
    "    summed_location = torch.sum(summed_features[:, len(le_species.classes_):len(le_species.classes_) + 2], dim=1)\n",
    "    mean_location = torch.mean(summed_location)\n",
    "    summed_altitude = torch.sum(summed_features[:, len(le_species.classes_) + 2:len(le_species.classes_) + 3], dim=1)\n",
    "    mean_altitude = torch.mean(summed_altitude)\n",
    "    summed_country = torch.sum(summed_features[:, len(le_species.classes_) + 3:len(le_species.classes_) + 3 + len(ohe_country.categories_[0])], dim=1)\n",
    "    mean_country = torch.mean(summed_country)\n",
    "    summed_ecoregion = torch.sum(summed_features[:, len(le_species.classes_) + 3 + len(ohe_country.categories_[0]):len(le_species.classes_) + 3 + len(ohe_country.categories_[0]) + len(ohe_ecoregion.categories_[0])], dim=1)\n",
    "    mean_ecoregion = torch.mean(summed_ecoregion)\n",
    "    summed_dune = torch.sum(summed_features[:, len(le_species.classes_) + 3 + len(ohe_country.categories_[0]) + len(ohe_ecoregion.categories_[0]):len(le_species.classes_) + 3 + len(ohe_country.categories_[0]) + len(ohe_ecoregion.categories_[0]) + len(ohe_dune.categories_[0])], dim=1)\n",
    "    mean_dune = torch.mean(summed_dune)\n",
    "    summed_coast = torch.sum(summed_features[:, len(le_species.classes_) + 3 + len(ohe_country.categories_[0]) + len(ohe_ecoregion.categories_[0]) + len(ohe_dune.categories_[0]):], dim=1)\n",
    "    mean_coast = torch.mean(summed_coast)\n",
    "    \n",
    "    scores[i].extend([mean_herbaceous, mean_arborescent, mean_location, mean_altitude, mean_country, mean_ecoregion, mean_dune, mean_coast])\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            [score[0:2] for score in scores],\n",
    "            index=index,\n",
    "            columns=[\"Herbaceous\", \"Arborescent\"]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            [score[2:] for score in scores],\n",
    "            index=index,\n",
    "            columns=[\"Location\", \"Altitude\", \"Country\", \"Ecoregion\", \"Dune\", \"Coast\"]\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    "    keys=[\"Internal\", \"External\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=go.Layout(\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        barmode=\"relative\",\n",
    "        yaxis_showticklabels=False,\n",
    "        yaxis_showgrid=False,\n",
    "        yaxis_range=[0, df.groupby(axis=1, level=0).sum().max().max() * 1.5],\n",
    "        yaxis2=go.layout.YAxis(\n",
    "            visible=False,\n",
    "            matches=\"y\",\n",
    "            overlaying=\"y\",\n",
    "            anchor=\"x\",\n",
    "        ),\n",
    "        font=dict(size=20),\n",
    "        legend_x=0,\n",
    "        legend_y=1,\n",
    "        legend_orientation=\"h\",\n",
    "        hovermode=\"x\",\n",
    "        margin=dict(b=0,t=10,l=0,r=10)\n",
    "    )\n",
    ")\n",
    "\n",
    "colors = {\n",
    "    \"Internal\": {\n",
    "        \"Herbaceous\": px.colors.sequential.Greens[3],\n",
    "        \"Arborescent\": px.colors.sequential.Greens[7]\n",
    "    },\n",
    "    \"External\": {\n",
    "        \"Location\": px.colors.sequential.Reds[3],\n",
    "        \"Altitude\": px.colors.sequential.Reds[4],\n",
    "        \"Country\": px.colors.sequential.Reds[5],\n",
    "        \"Ecoregion\": px.colors.sequential.Reds[6],\n",
    "        \"Dune\": px.colors.sequential.Reds[7],\n",
    "        \"Coast\": px.colors.sequential.Reds[8]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add the traces\n",
    "for i, t in enumerate(colors):\n",
    "    for j, col in enumerate(df[t].columns):\n",
    "        if (df[t][col] == 0).all():\n",
    "            continue\n",
    "        fig.add_bar(\n",
    "            x=df.index,\n",
    "            y=df[t][col],\n",
    "            yaxis=f\"y{i + 1}\",\n",
    "            offsetgroup=str(i),\n",
    "            offset=(i - 1) * 1/3,\n",
    "            width=1/3,\n",
    "            legendgroup=t,\n",
    "            legendgrouptitle_text=t,\n",
    "            name=col,\n",
    "            marker_color=colors[t][col],\n",
    "            marker_line=dict(width=2, color=\"#333\"),\n",
    "            hovertemplate=\"%{y}<extra></extra>\"\n",
    "        )\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/internal_external_per_habitat.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfce6c",
   "metadata": {},
   "source": [
    "## Importance by rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402590af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = torch.load(os.path.join(framework_directory, 'Experiments/ranks.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "\n",
    "x = np.arange(1, len(le_species.classes_) + 1)[:50]\n",
    "y = ranks[:50].cpu()\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=x, y=y))\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Species dominance rank\", yaxis_title=\"Mean attribution\")\n",
    "fig.update_xaxes(range=[0, 51])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/importance_by_rank.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242769da",
   "metadata": {},
   "source": [
    "## Feature ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablations = torch.load(os.path.join(framework_directory, 'Experiments/ablations.pt'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "\n",
    "data = {\n",
    "    \"Habitat\": [habitat[:-2] for habitat in le_header.classes_.tolist() * 2],  # Repeat each class twice\n",
    "    \"Criteria\": [\"Internal\"] * 228 + [\"External\"] * 228,\n",
    "    \"Ablations\": np.concatenate((ablations[:, 0], ablations[:, 1]))  # Concatenate the values from ablations\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig = px.box(df, x='Habitat', y='Ablations', color='Criteria', height=500, width=1000)\n",
    "\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/feature_ablation_per_habitat.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201fa91",
   "metadata": {},
   "source": [
    "## Micro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a962f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds=[f\"Fold {i}\" for i in range(10)]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Top-3', x=folds, y=[98.63, 98.59, 98.50, 98.54, 98.51, 98.52, 98.58, 98.56, 98.42, 98.64], marker_color='red'),\n",
    "    go.Bar(name='Top-1', x=folds, y=[89.06, 89.10, 88.13, 88.88, 89.08, 88.80, 88.91, 88.80, 88.22, 88.47], marker_color='blue')\n",
    "])\n",
    "\n",
    "fig.update_layout(barmode='overlay',\n",
    "                  xaxis_title='Folds',\n",
    "                  yaxis_title='Accuracy (%)',\n",
    "                  showlegend=False,\n",
    "                  height=500,\n",
    "                  width=1000)\n",
    "\n",
    "fig.update_yaxes(range=[50, 100])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/micro_average.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dabb9",
   "metadata": {},
   "source": [
    "## Macro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds=[f\"Fold {i}\" for i in range(10)]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Top-3', x=folds, y=[91.89, 93.62, 90.91, 90.39, 88.94, 93.49, 91.57, 89.02, 88.56, 89.71], marker_color='red'),\n",
    "    go.Bar(name='Top-1', x=folds, y=[74.08, 77.23, 73.64, 72.59, 72.82, 77.10, 74.06, 73.00, 73.62, 71.58], marker_color='blue')\n",
    "])\n",
    "\n",
    "fig.update_layout(barmode='overlay',\n",
    "                  xaxis_title='Folds',\n",
    "                  yaxis_title='Accuracy (%)',\n",
    "                  showlegend=False,\n",
    "                  height=500,\n",
    "                  width=1000)\n",
    "\n",
    "fig.update_yaxes(range=[50, 100])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(framework_directory, 'Images/macro_average.pdf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
