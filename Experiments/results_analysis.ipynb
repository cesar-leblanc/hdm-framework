{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3fddc5",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef857774",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c59f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import geopandas as gpd\n",
    "import rtree\n",
    "import shapely\n",
    "import random\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac382b",
   "metadata": {},
   "source": [
    "## Paths creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff85832",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_directory = os.path.dirname(os.path.abspath('__file__'))\n",
    "framework_directory = os.path.abspath(os.path.join(notebook_directory, '..'))\n",
    "\n",
    "sys.path.append(framework_directory)\n",
    "\n",
    "print(framework_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a8494",
   "metadata": {},
   "source": [
    "## Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d0622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/test_values.txt'), \"r\") as file:\n",
    "    y_test = file.readlines()\n",
    "    y_test = [line.strip() for line in y_test]\n",
    "y_test = [element.split(', ') for element in y_test]\n",
    "\n",
    "with open(os.path.join(framework_directory, 'Data/predictions.txt'), \"r\") as file:\n",
    "    predictions = file.readlines()\n",
    "    predictions = [line.strip() for line in predictions]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] in y_test[i]:\n",
    "        count +=1\n",
    "\n",
    "print(f\"hdm-framework test accuracy: {round(count/len(predictions)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6787bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i][:-1] in [y_test[i][j][:-1] for j in range(len(y_test[i]))]:\n",
    "        count +=1\n",
    "\n",
    "print(f\"hdm-framework test accuracy (level 2): {round(count/len(predictions)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f42d1",
   "metadata": {},
   "source": [
    "## Expert system comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header_esy = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'))\n",
    "\n",
    "test_header_esy = test_header_esy.rename(columns={'PlotObservationID': 'RELEVE_NR', 'Altitude': 'Altitude (m)', 'Longitude': 'DEG_LON', 'Latitude': 'DEG_LAT', 'Ecoregion': 'Ecoreg', 'Dune': 'Dunes_Bohn', 'Coast': 'Coast_EEA'})\n",
    "test_header_esy['GESELLSCH'] = 'GESELLSCH'\n",
    "test_header_esy['dataset'] = 'dataset'\n",
    "test_header_esy = test_header_esy[['RELEVE_NR', 'Country', 'Altitude (m)', 'DEG_LON', 'DEG_LAT', 'GESELLSCH', 'dataset', 'Ecoreg', 'Dunes_Bohn', 'Coast_EEA']]\n",
    "\n",
    "test_header_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_header_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "test_header_esy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57882cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/eunis-esy_predictions.csv'))\n",
    "\n",
    "predictions_esy = predictions_esy.rename(columns={'x': 'PREDICTION'})\n",
    "predictions_esy.index = test_header_esy['RELEVE_NR']\n",
    "\n",
    "predictions_esy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(predictions_esy)):\n",
    "    if predictions_esy.PREDICTION.values[i] in y_test[i]:\n",
    "        count +=1\n",
    "\n",
    "print(f\"EUNIS-ESy test accuracy: {round(count/len(predictions_esy)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cf02f",
   "metadata": {},
   "source": [
    "## Regions comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_kingdom_regions_columns = ['rgn_name', 'geometry']\n",
    "\n",
    "united_kingdom_regions = gpd.read_file(os.path.join(framework_directory, 'Datasets/united_kingdom_regions.shp'))\n",
    "united_kingdom_regions = united_kingdom_regions[united_kingdom_regions_columns]\n",
    "united_kingdom_regions = united_kingdom_regions.rename(columns={'rgn_name': 'region'})\n",
    "united_kingdom_regions['region'] = united_kingdom_regions['region'].apply(lambda x: ''.join(char for char in x if char not in [\"'\", \"[\", \"]\"]))\n",
    "united_kingdom_regions = united_kingdom_regions.to_crs(crs=3857)\n",
    "\n",
    "united_kingdom_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e318974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'))\n",
    "\n",
    "united_kingdom_region_index = rtree.index.Index()  # Create a spatial index for the United Kingdom region boundaries\n",
    "for index, united_kingdom_region in united_kingdom_regions.iterrows():  # Insert each United Kingdom region's index and bounding box into the spatial index\n",
    "    united_kingdom_region_index.insert(index, united_kingdom_region[\"geometry\"].bounds)\n",
    "list_of_united_kingdom_regions = []   # Initialize a list to store the United Kingdom region labels\n",
    "for i in tqdm.tqdm(range(len(test_header)), desc=\"United Kingdom regions\"):  # Iterate over each row in the test_header DataFrame\n",
    "    point = shapely.geometry.Point(test_header['Longitude'][i], test_header['Latitude'][i])\n",
    "    point = {'geometry': [point]}  # Create a point geometry from the longitude and latitude\n",
    "    point = gpd.GeoDataFrame(point, crs=\"EPSG:4326\")  # Convert the point to a GeoDataFrame with EPSG:4326 CRS\n",
    "    point = point.to_crs(crs=3857)  # Convert the point's CRS to EPSG:3857 (Web Mercator)\n",
    "    point = pd.Series(data=point['geometry'][0], index=['geometry'])  # Extract the geometry of the point as a Pandas Series\n",
    "    min_distance = float(\"inf\")  # Initialize variables for finding the closest United Kingdom region\n",
    "    closest_united_kingdom_region = None\n",
    "    for united_kingdom_region_id in united_kingdom_region_index.nearest((point[\"geometry\"].x, point[\"geometry\"].y, point[\"geometry\"].x, point[\"geometry\"].y), 1):  # Find United Kingdom regions that are within a certain distance of the point\n",
    "        distance = point[\"geometry\"].distance(united_kingdom_regions.iloc[united_kingdom_region_id][\"geometry\"])  # Calculate the distance between the point and the United Kingdom region\n",
    "        if distance < min_distance:  # Update the closest United Kingdom region if the distance is smaller\n",
    "            min_distance = distance\n",
    "            closest_united_kingdom_region = united_kingdom_regions.iloc[united_kingdom_region_id]\n",
    "    closest_united_kingdom_region = closest_united_kingdom_region[0]  # Get the name of the closest United Kingdom region\n",
    "    list_of_united_kingdom_regions.append(closest_united_kingdom_region)  # Append the closest United Kingdom region to the list of United Kingdom regions\n",
    "test_header['Region'] = list_of_united_kingdom_regions  # Add the United Kingdom region labels to df_header\n",
    "\n",
    "test_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_header_united_kingdom = pd.read_csv(os.path.join(framework_directory, 'Datasets/eva_header.csv'))\n",
    "train_header_united_kingdom = train_header_united_kingdom[train_header_united_kingdom['Country'] == 'United Kingdom'].reset_index(drop=True)\n",
    "\n",
    "list_of_united_kingdom_regions = []   # Initialize a list to store the United Kingdom region labels\n",
    "for i in tqdm.tqdm(range(len(train_header_united_kingdom)), desc=\"United Kingdom regions\"):  # Iterate over each row in the test_header DataFrame\n",
    "    point = shapely.geometry.Point(train_header_united_kingdom['Longitude'][i], train_header_united_kingdom['Latitude'][i])\n",
    "    point = {'geometry': [point]}  # Create a point geometry from the longitude and latitude\n",
    "    point = gpd.GeoDataFrame(point, crs=\"EPSG:4326\")  # Convert the point to a GeoDataFrame with EPSG:4326 CRS\n",
    "    point = point.to_crs(crs=3857)  # Convert the point's CRS to EPSG:3857 (Web Mercator)\n",
    "    point = pd.Series(data=point['geometry'][0], index=['geometry'])  # Extract the geometry of the point as a Pandas Series\n",
    "    min_distance = float(\"inf\")  # Initialize variables for finding the closest United Kingdom region\n",
    "    closest_united_kingdom_region = None\n",
    "    for united_kingdom_region_id in united_kingdom_region_index.nearest((point[\"geometry\"].x, point[\"geometry\"].y, point[\"geometry\"].x, point[\"geometry\"].y), 1):  # Find United Kingdom regions that are within a certain distance of the point\n",
    "        distance = point[\"geometry\"].distance(united_kingdom_regions.iloc[united_kingdom_region_id][\"geometry\"])  # Calculate the distance between the point and the United Kingdom region\n",
    "        if distance < min_distance:  # Update the closest United Kingdom region if the distance is smaller\n",
    "            min_distance = distance\n",
    "            closest_united_kingdom_region = united_kingdom_regions.iloc[united_kingdom_region_id]\n",
    "    closest_united_kingdom_region = closest_united_kingdom_region[0]  # Get the name of the closest United Kingdom region\n",
    "    list_of_united_kingdom_regions.append(closest_united_kingdom_region)  # Append the closest United Kingdom region to the list of United Kingdom regions\n",
    "train_header_united_kingdom['Region'] = list_of_united_kingdom_regions  # Add the United Kingdom region labels to df_header\n",
    "\n",
    "train_header_united_kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in test_header['Region'].unique():\n",
    "    regions_indices = test_header[test_header['Region'] == region].index\n",
    "    filtered_predictions = [predictions[i] for i in regions_indices]\n",
    "    filtered_predictions_esy = [predictions_esy['PREDICTION'].to_list()[i] for i in regions_indices]\n",
    "    filtered_y_test = [y_test[i] for i in regions_indices]\n",
    "    count_framework = 0\n",
    "    count_esy = 0\n",
    "    for i in range(len(filtered_predictions)):\n",
    "        if filtered_predictions[i] in filtered_y_test[i]:\n",
    "            count_framework +=1\n",
    "        if filtered_predictions_esy[i] in filtered_y_test[i]:\n",
    "            count_esy += 1\n",
    "    print(f\"Number of plots in {region} in the training set: {len(train_header_united_kingdom[train_header_united_kingdom['Region'] == region])}\")\n",
    "    print(f\"Number of plots in {region} in the test set: {len(test_header[test_header['Region'] == region])}\")\n",
    "    print(f\"hdm-framework test accuracy in {region}: {round(count_framework/len(filtered_predictions)*100, 2)}%\")\n",
    "    print(f\"EUNIS-ESy test accuracy in {region}: {round(count_esy/len(filtered_predictions_esy)*100, 2)}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5427e0d",
   "metadata": {},
   "source": [
    "## Data requirements comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fe4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header_esy['Country'] = 'Country'\n",
    "test_header_esy['Altitude (m)'] = 'Altitude (m)'\n",
    "test_header_esy['DEG_LON'] = 'DEG_LON'\n",
    "test_header_esy['DEG_LAT'] = 'DEG_LAT'\n",
    "test_header_esy['Ecoreg'] = 'Ecoreg'\n",
    "test_header_esy['Dunes_Bohn'] = 'Dunes_Bohn'\n",
    "test_header_esy['Coast_EEA'] = 'Coast_EEA'\n",
    "\n",
    "test_header_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_header_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "test_header_esy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f150491",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/predictions_MLP_rank_species.txt'), \"r\") as file:\n",
    "    predictions = file.readlines()\n",
    "    predictions = [line.strip() for line in predictions]\n",
    "\n",
    "predictions_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/eunis-esy_predictions_species.csv'))\n",
    "\n",
    "predictions_esy = predictions_esy.rename(columns={'x': 'PREDICTION'})\n",
    "predictions_esy.index = test_header_esy['RELEVE_NR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_framework = 0\n",
    "count_esy = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if predictions[i] in y_test[i]:\n",
    "        count_framework +=1\n",
    "    if predictions_esy.PREDICTION.values[i] in y_test[i]:\n",
    "        count_esy += 1\n",
    "\n",
    "print(f\"hdm-framework test accuracy (with only plant species composition): {round(count_framework/len(predictions)*100, 2)}%\")\n",
    "print(f\"EUNIS-ESy test accuracy (with only plant species composition): {round(count_esy/len(predictions_esy)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8180bdc6",
   "metadata": {},
   "source": [
    "## Representation learning comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd14c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_species_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'))\n",
    "\n",
    "test_species_esy['Cover_Perc'] = 10\n",
    "\n",
    "test_species_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "test_species_esy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/predictions_MLP_binarization_all.txt'), \"r\") as file:\n",
    "    predictions = file.readlines()\n",
    "    predictions = [line.strip() for line in predictions]\n",
    "\n",
    "predictions_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/eunis-esy_predictions_binarization.csv'))\n",
    "\n",
    "predictions_esy = predictions_esy.rename(columns={'x': 'PREDICTION'})\n",
    "predictions_esy.index = test_header_esy['RELEVE_NR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e05cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_framework = 0\n",
    "count_esy = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if predictions[i] in y_test[i]:\n",
    "        count_framework +=1\n",
    "    if predictions_esy.PREDICTION.values[i] in y_test[i]:\n",
    "        count_esy += 1\n",
    "\n",
    "print(f\"hdm-framework test accuracy (with presence-absence data): {round(count_framework/len(predictions)*100, 2)}%\")\n",
    "print(f\"EUNIS-ESy test accuracy (with presence-absence data): {round(count_esy/len(predictions_esy)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae976eef",
   "metadata": {},
   "source": [
    "## Noise robustness comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_species_framework = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_species.csv'), delimiter='\\t')\n",
    "test_species_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'))\n",
    "\n",
    "sample_size_framework = int(len(test_species_framework) * 0.3)\n",
    "sample_indices_framework = random.sample(list(test_species_framework.index.tolist()), sample_size_framework)\n",
    "test_species_framework.loc[sample_indices_framework, 'Cover %'] = 0\n",
    "test_species_framework = test_species_framework.reset_index(drop=True)\n",
    "\n",
    "sample_size_esy = int(len(test_species_esy) * 0.3)\n",
    "sample_indices_esy = random.sample(list(test_species_esy.index.tolist()), sample_size_esy)\n",
    "test_species_esy.loc[sample_indices_esy, 'Cover_Perc'] = 0\n",
    "test_species_esy = test_species_esy.reset_index(drop=True)\n",
    "\n",
    "test_species_framework.to_csv(os.path.join(framework_directory, 'Datasets/test_species.csv'), index=False, sep='\\t')\n",
    "test_species_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd77ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/predictions_MLP_rank_all_noise.txt'), \"r\") as file:\n",
    "    predictions = file.readlines()\n",
    "    predictions = [line.strip() for line in predictions]\n",
    "\n",
    "predictions_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/eunis-esy_predictions_dropout.csv'))\n",
    "\n",
    "predictions_esy = predictions_esy.rename(columns={'x': 'PREDICTION'})\n",
    "predictions_esy.index = test_header_esy['RELEVE_NR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_framework = 0\n",
    "count_esy = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if predictions[i] in y_test[i]:\n",
    "        count_framework +=1\n",
    "    if predictions_esy.PREDICTION.values[i] in y_test[i]:\n",
    "        count_esy += 1\n",
    "\n",
    "print(f\"hdm-framework test accuracy (with 30% dropout): {round(count_framework/len(predictions)*100, 2)}%\")\n",
    "print(f\"EUNIS-ESy test accuracy (with 30% dropout): {round(count_esy/len(predictions_esy)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9894cc",
   "metadata": {},
   "source": [
    "## First vegetation plot retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header_framework = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'), delimiter='\\t')\n",
    "test_header_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_header_esy.csv'))\n",
    "test_species_framework = pd.read_csv(os.path.join(framework_directory, 'Datasets/test_species.csv'), delimiter='\\t')\n",
    "test_species_esy = pd.read_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'))\n",
    "\n",
    "first_plot = test_header_framework.loc[0]['PlotObservationID']\n",
    "\n",
    "test_header_framework = test_header_framework[test_header_framework['PlotObservationID'] == first_plot]\n",
    "test_header_esy = test_header_esy[test_header_esy['RELEVE_NR'] == first_plot]\n",
    "test_species_framework = test_species_framework[test_species_framework['PlotObservationID'] == first_plot]\n",
    "test_species_esy = test_species_esy[test_species_esy['RELEVE_NR'] == first_plot]\n",
    "\n",
    "test_header_framework.to_csv(os.path.join(framework_directory, 'Datasets/test_header.csv'), index=False, sep='\\t')\n",
    "test_header_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_header_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "test_species_framework.to_csv(os.path.join(framework_directory, 'Datasets/test_species.csv'), index=False, sep='\\t')\n",
    "test_species_esy.to_csv(os.path.join(framework_directory, 'Experiments/ESy/data/test_species_esy.csv'), index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a71554",
   "metadata": {},
   "source": [
    "## Top-k feature importance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/ohe_country.pkl'), 'rb') as f:\n",
    "    ohe_country = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_ecoregion.pkl'), 'rb') as f:\n",
    "    ohe_ecoregion = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_dune.pkl'), 'rb') as f:\n",
    "    ohe_dune = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_coast.pkl'), 'rb') as f:\n",
    "    ohe_coast = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "attributions = torch.load(\"attributions.pt\")\n",
    "\n",
    "feature_names = le_species.classes_.tolist() + [\"Longitude\"] + [\"Latitude\"] + [\"Altitude\"] + ohe_country.categories_[0].tolist() + [f\"Ecoregion {ecoregion}\" for ecoregion in ohe_ecoregion.categories_[0].tolist()] + ohe_dune.categories_[0].tolist() + ohe_coast.categories_[0].tolist()\n",
    "\n",
    "mean_attributions = torch.mean(attributions, dim=0)\n",
    "\n",
    "k = 10\n",
    "\n",
    "sorted_indices = torch.argsort(mean_attributions, descending=True)\n",
    "top_k_indices = sorted_indices[:k]\n",
    "top_k_scores = mean_attributions[top_k_indices]\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Feature: {feature_names[top_k_indices[i]]}, Score: {top_k_scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b40214",
   "metadata": {},
   "source": [
    "## Top-k feature importance per habitat calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4176f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Data/ohe_country.pkl'), 'rb') as f:\n",
    "    ohe_country = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_ecoregion.pkl'), 'rb') as f:\n",
    "    ohe_ecoregion = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_dune.pkl'), 'rb') as f:\n",
    "    ohe_dune = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/ohe_coast.pkl'), 'rb') as f:\n",
    "    ohe_coast = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "attributions = torch.load(\"attributions.pt\")\n",
    "\n",
    "feature_names = le_species.classes_.tolist() + [\"Longitude\"] + [\"Latitude\"] + [\"Altitude\"] + ohe_country.categories_[0].tolist() + [f\"Ecoregion {ecoregion}\" for ecoregion in ohe_ecoregion.categories_[0].tolist()] + ohe_dune.categories_[0].tolist() + ohe_coast.categories_[0].tolist()\n",
    "\n",
    "k = 10\n",
    "\n",
    "habitat = 'R22'\n",
    "habitat_idx = le_header.transform([habitat])[0]\n",
    "\n",
    "habitat_attributions = attributions[habitat_idx]\n",
    "\n",
    "sorted_indices = torch.argsort(habitat_attributions, descending=True)\n",
    "top_k_indices = sorted_indices[:k]\n",
    "top_k_scores = habitat_attributions[top_k_indices]\n",
    "\n",
    "print(f\"{k}-most important features for the habitat {habitat}:\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Feature: {feature_names[top_k_indices[i]]}, Score: {top_k_scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d1ad2",
   "metadata": {},
   "source": [
    "## Most important species per habitat type calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = torch.load('attributions.pt')\n",
    "with open(os.path.join(framework_directory, 'Data/le_species.pkl'), 'rb') as f:\n",
    "    le_species = pickle.load(f)\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "    \n",
    "nbr_habitats = np.zeros(8, dtype=int)\n",
    "habitat_types = ['MA2', 'N', 'Q', 'R', 'S', 'T', 'U', 'V']\n",
    "\n",
    "for code in le_header.classes_:\n",
    "    for i, habitat in enumerate(habitat_types):\n",
    "        if code.startswith(habitat):\n",
    "            nbr_habitats[i] += 1\n",
    "            break\n",
    "\n",
    "genera = np.asarray([species.split()[0] for species in le_species.classes_])\n",
    "unique_genera = np.unique(genera)\n",
    "attributions_genera = torch.zeros((228, len(unique_genera)))\n",
    "attributions_species = attributions[:, :len(le_species.classes_)]\n",
    "species_per_habitat_type = torch.zeros((8, len(le_species.classes_)))\n",
    "genera_per_habitat_type = torch.zeros((8, len(unique_genera)))\n",
    "\n",
    "for i in range(len(le_species.classes_)):\n",
    "    for j in range(len(unique_genera)):\n",
    "        if le_species.classes_[i].split()[0] == unique_genera[j]:\n",
    "            attributions_genera[:, j] += attributions_species[:, i]\n",
    "            break\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "for i in range(len(nbr_habitats)):\n",
    "    end_idx = start_idx + nbr_habitats[i]\n",
    "    summed_species = attributions_species[start_idx:end_idx]\n",
    "    summed_genera = attributions_genera[start_idx:end_idx]\n",
    "    mean_species = torch.mean(summed_species, axis=0)\n",
    "    mean_genera = torch.mean(summed_genera, axis=0)\n",
    "    species_per_habitat_type[i] = mean_species\n",
    "    genera_per_habitat_type[i] = mean_genera\n",
    "    start_idx = end_idx\n",
    "    \n",
    "    idx_max_species = torch.argsort(species_per_habitat_type[i], descending=True)[:3]\n",
    "    species_max = le_species.inverse_transform(idx_max_species.numpy())\n",
    "    idx_max_genera = torch.argsort(genera_per_habitat_type[i], descending=True)[0]\n",
    "    genera_max = unique_genera[idx_max_genera]\n",
    "    print(f\"Three most important species for habitat type {habitat_types[i]} are:\\n1 - {species_max[0]}\\n2 - {species_max[1]}\\n3 - {species_max[2]}\")\n",
    "    print(f\"The most important genera is: {genera_max}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0a288",
   "metadata": {},
   "source": [
    "## Ranks importance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = torch.load(os.path.join(framework_directory, 'Experiments/ranks.pt'))\n",
    "\n",
    "total_sum = torch.sum(ranks)\n",
    "\n",
    "target_sum = total_sum * 0.5\n",
    "\n",
    "current_sum = 0\n",
    "count = 0\n",
    "\n",
    "for value in ranks:\n",
    "    current_sum += value\n",
    "    count += 1\n",
    "    if current_sum >= target_sum:\n",
    "        break\n",
    "\n",
    "print(f\"To reach 50% of the total interpretability, we should take into account the first {count} species of each vegetation plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74698d",
   "metadata": {},
   "source": [
    "## Unambiguous and ambiguous classes comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203882a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(framework_directory, 'Experiments/predictions.pkl'), 'rb') as file:\n",
    "    predictions = pickle.load(file)\n",
    "target_values = np.load(os.path.join(framework_directory, 'Data/target_values.npy'))\n",
    "split_assignments = np.load(os.path.join(framework_directory, 'Data/split_assignments.npy'))\n",
    "with open(os.path.join(framework_directory, 'Data/le_header.pkl'), 'rb') as f:\n",
    "    le_header = pickle.load(f)\n",
    "\n",
    "target_values = np.concatenate([target_values[split_assignments == i] for i in range(10)])\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "conf_matrix = sklearn.metrics.confusion_matrix(target_values, predictions)\n",
    "class_accuracy = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "\n",
    "most_unambiguous_class = np.argmax(class_accuracy)\n",
    "most_ambiguous_class = np.argmin(class_accuracy)\n",
    "\n",
    "percentage_most_unambiguous = class_accuracy[most_unambiguous_class] * 100\n",
    "percentage_most_ambiguous = class_accuracy[most_ambiguous_class] * 100\n",
    "\n",
    "most_unambiguous_class = le_header.inverse_transform([most_unambiguous_class])[0]\n",
    "most_ambiguous_class = le_header.inverse_transform([most_ambiguous_class])[0]\n",
    "\n",
    "print(f\"Habitat that the model identifies most confidently: {most_unambiguous_class}\")\n",
    "print(f\"Percentage of correct predictions for {most_unambiguous_class}: {percentage_most_unambiguous:.2f}%\\n\")\n",
    "\n",
    "print(f\"Habitat that the model has the most difficulty with: {most_ambiguous_class}\")\n",
    "print(f\"Percentage of correct predictions for {most_ambiguous_class}: {percentage_most_ambiguous:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
